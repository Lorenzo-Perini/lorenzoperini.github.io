[{"authors":["admin"],"categories":null,"content":"Lorenzo Perini is a researcher focusing on the intersection of probability and machine learning. He earned his BSc in Mathematics from the University of Florence in 2017, and his MSc in Mathematical Engineering from Politecnico di Torino in 2019, where he specialized in data statistics and network optimization. His Master\u0026rsquo;s thesis, conducted in collaboration with Tierra S.p.A., explored predictive maintenance using Hidden Markov Models and Autoencoders.\nIn 2019, Lorenzo began his PhD at KU Leuven\u0026lsquo;s DTAI Lab under Prof. Dr. Jesse Davis, focusing on uncertainty quantification and anomaly detection. His work has been recognized with several fellowships, including a PhD fellowship from the Research Foundation – Flanders (FWO) and the Scientific Prize Gustave Boël-Sofina Fellowship for talented researchers for a long stay abroad. During his PhD, he was a visiting researcher at the University of Helsinki and completed an internship at Bosch Center for Artificial Intelligence (BCAI).\nLorenzo has published papers in prestigious conferences such as NeurIPS, ICML, KDD, AAAI, IJCAI, and ECAI. His main research interests include Uncertainty Quantification and Anomaly Detection, often with the introduction of the human-in-the-loop, e.g. via Active Learning and Learning to Reject. For further information about his research, feel free to check out his research mission. Having recently defended his doctoral dissertation on operational, uncertainty-aware, and reliable anomaly detection, he is now seeking research opportunities in industry.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://lorenzoperini.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Lorenzo Perini is a researcher focusing on the intersection of probability and machine learning. He earned his BSc in Mathematics from the University of Florence in 2017, and his MSc in Mathematical Engineering from Politecnico di Torino in 2019, where he specialized in data statistics and network optimization. His Master\u0026rsquo;s thesis, conducted in collaboration with Tierra S.p.A., explored predictive maintenance using Hidden Markov Models and Autoencoders.\nIn 2019, Lorenzo began his PhD at KU Leuven\u0026lsquo;s DTAI Lab under Prof.","tags":null,"title":"Lorenzo Perini","type":"authors"},{"authors":["Lorenzo Perini"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"ecd92f6d7c0c32175d9d5333dba47fe0","permalink":"https://lorenzoperini.github.io/publication/phdthesis/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/publication/phdthesis/","section":"publication","summary":"Anomaly detection methods aim to identify examples that do not follow the expected behavior. For various reasons, anomaly detection is typically tackled by using unsupervised approaches that assign real-valued anomaly scores based on various heuristics. For instance, one can assume that anomalies fall in low-density regions and compute the negative log-likelihood as anomaly score. Because anomaly scores are often hard to interpret, practitioners need class labels (i.e., anomaly yes/no) for decision-making. That is, one needs to set a proper decision threshold to flag high-score examples as anomalies. However, finding a threshold requires having access to labeled examples for evaluating the quality of the predicted class labels, which is unfeasible in unsupervised anomaly detection. Moreover, existing literature has focused mainly on measuring the quality of the anomaly scores through ranking-based metrics (e.g., AUROC), which largely ignores the problem of how to derive class predictions. Here, we fill this gap by proposing three novel approaches to transform scores into class predictions. Given a detector’s class predictions, a natural question is: how likely does a prediction change when learning a detector on training data that is subject to slight perturbation? Because unsupervised detectors cannot refine the decision boundary by leveraging labeled examples, they tend to have high uncertainty in predictions. That is, slight changes in the training set often would yield a different decision boundary which, in turn, would flip some test examples’ class prediction. This uncertainty makes it hard to deploy a detector in real-world applications as it deteriorates the practitioner’s trust in its crucial predictions. Because existing literature largely ignores this problem, we fill this gap by proposing an unsupervised approach to quantify a detector’s uncertainty in predictions. While quantifying uncertainty is essential, practitioners also need a reliable way to assess whether they can trust a detector’s prediction. That is, one needs to answer the question: is the detector’s uncertainty low enough to rely on its prediction? This falls into the field of Learning with Rejection, where the model is allowed to abstain (i.e., defer the decision, or “reject” it) when its uncertainty is too high, such that practitioners can trust its output whenever it makes a prediction. Traditionally, learning with rejection approaches rely on evaluating the risk (or, equivalently, the cost) of making mispredictions to design the rejection mechanism, which requires labeled examples. Because no unsupervised method for rejection exists, we fill this gap and propose the first unsupervised anomaly detection algorithm with rejection.","tags":["Anomaly Detection","PU Learning","Uncertainty Quantification","Bayesian Learning","Transfer Learning","Active Learning","Learning to Reject","Unsupervised Learning"],"title":"Operational, Uncertainty-Aware, and Reliable Anomaly Detection","type":"publication"},{"authors":["Kilian Hendrickx","Lorenzo Perini","Dries Van der Plas","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1711670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711670400,"objectID":"c8c0606d84b0c95883d7afb9e8d4e225","permalink":"https://lorenzoperini.github.io/publication/mach_rejection/","publishdate":"2024-03-29T00:00:00Z","relpermalink":"/publication/mach_rejection/","section":"publication","summary":"Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with a reject option recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with a reject option. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection. Moreover, we define the existing architectures for models with a reject option, describe the standard learning strategies to train such models and relate traditional machine learning techniques to rejection. Additionally, we review strategies to evaluate a model's predictive and rejective quality. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.","tags":["Machine Learning with Rejection","Supervised Learning","Trustworthy Machine Learning"],"title":"Machine Learning with a Reject Option: A survey","type":"publication"},{"authors":["Lorenzo Perini","Jesse Davis"],"categories":[],"content":"","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"97b8f5f491b5e1ba68ae85c49d02486d","permalink":"https://lorenzoperini.github.io/publication/neurips23/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/neurips23/","section":"publication","summary":"Anomaly detection aims at detecting unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, that may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of such a metric. Moreover, setting a constant threshold results in strong guarantees: we estimate the test rejection rate, and derive a theoretical upper bound for both the rejection rate and the expected prediction cost. Experimentally, we show that our method outperforms some metric-based methods.","tags":["Anomaly Detection","Learning to Reject","Unsupervised Learning"],"title":"Unsupervised Anomaly Detection with Rejection","type":"publication"},{"authors":["Laurens Devos","Lorenzo Perini","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"c28f3acf314bf0e75d2d0c4ec464bc85","permalink":"https://lorenzoperini.github.io/publication/ecml2023a/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/ecml2023a/","section":"publication","summary":"Tree ensembles are powerful models that are widely used. However, they are susceptible to evasion attacks where an adversary purposely constructs an adversarial example in order to elicit a misprediction from the model. This can degrade performance and erode a user’s trust in the model. Typically, approaches try to alleviate this problem by verifying how robust a learned ensemble is or robustifying the learning process. We take an alternative approach and attempt to detect adversarial examples in a post-deployment setting. We present a novel method for this task that works by analyzing an unseen example’s output configuration, which is the set of leaves activated by the example in the ensemble’s constituent trees. Our approach works with any additive tree ensemble and does not require training a separate model. We evaluate our approach on three different tree ensemble learners. We empirically show that our method is currently the best adversarial detection method for tree ensembles.","tags":["Evasion attack detection","Tree ensembles"],"title":"Detecting Evasion Attacks in Deployed Tree Ensembles","type":"publication"},{"authors":["Timo Martens","Lorenzo Perini","Jesse Davis"],"categories":[],"content":"","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"2d470ba3f02c01d7262ec385089b9df9","permalink":"https://lorenzoperini.github.io/publication/ecml2023b/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/ecml2023b/","section":"publication","summary":"Anomaly detection aims at detecting examples that do not conform to normal behavior. Increasingly, anomaly detection is being approached from a semi-supervised perspective where active learning is employed to acquire a small number of strategically selected labels. However, because anomalies are not always well-understood events, the user may be uncertain about how to label certain instances. Thus, one can relax this request and allow the user to provide soft labels (i.e., probabilistic labels) that represent their belief that a queried example is anomalous. These labels are naturally noisy due to the user’s inherent uncertainty in the label and the fact that people are known to be bad at providing well-calibrated probability instances. To cope with these challenges, we propose to exploit a Gaussian Process to learn from actively acquired soft labels in the context of anomaly detection. This enables leveraging information about nearby examples to smooth out possible noise. Empirically, we compare our proposed approach to several baselines on 21 datasets and show that it outperforms them in the majority of experiments.","tags":["Anomaly Detection","Probabilistic Labels","Noisy Labels"],"title":"Semi-Supervised Learning from Active Noisy Soft Labels for Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1691193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691193600,"objectID":"f98737cb9d38116c09ef1c913a796517","permalink":"https://lorenzoperini.github.io/publication/kdd23/","publishdate":"2023-08-05T00:00:00Z","relpermalink":"/publication/kdd23/","section":"publication","summary":"In the multi-instance learning (MIL) setting instances are grouped together into bags. Labels are provided only for the bags and not on the level of individual instances. A positive bag label means that at least one instance inside the bag is positive, while a negative bag label restricts all the instances in the bag to be negative. MIL data naturally arises in many contexts, such as anomaly detection, where labels are rare and costly, and one often ends up annotating the label for sets of instances. Moreover, in many real-world anomaly detection problems, only positive labels are collected because they usually represent critical events. Such a setting, where only positive labels are provided along with unlabeled data, is called Positive and Unlabeled (PU) learning. Despite being useful for several use cases, there is no work dedicated to learning from positive and unlabeled data in a multi-instance setting for anomaly detection. Therefore, we propose the first method that learns from PU bags in anomaly detection. Our method uses an autoencoder as an underlying anomaly detector. We alter the autoencoder’s objective function and propose a new loss that allows it to learn from positive and unlabeled bags of instances. We theoretically analyze this method. Experimentally, we evaluate our method on 30 datasets and show that it performs better than multiple baselines adapted to work in our setting.","tags":["Multi-Instance Learning","PU Learning","Anomaly Detection"],"title":"Learning from Positive and Unlabeled Multi-Instance Bags in Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Paul-Christian Buerkner","Arto Klami"],"categories":[],"content":"","date":1689984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689984000,"objectID":"d65d20f9b5d1e1b3cd6d9879f48bc665","permalink":"https://lorenzoperini.github.io/publication/icml23/","publishdate":"2023-09-02T00:00:00Z","relpermalink":"/publication/icml23/","section":"publication","summary":"Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor for a given unlabeled dataset. We leverage several anomaly detectors to capture the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the posterior mean improves the detectors’ performance over several alternative methods.","tags":["Anomaly Detection","Bayesian Methods","Unsupervised Learning"],"title":"Estimating the Contamination Factor’s Distribution in Unsupervised Anomaly Detection","type":"publication"},{"authors":[],"categories":null,"content":"","date":1676291400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676291400,"objectID":"d3f6c7909db7745f1037e0cb11e5a977","permalink":"https://lorenzoperini.github.io/talk/aaai/","publishdate":"2023-02-13T13:30:00+01:00","relpermalink":"/talk/aaai/","section":"talk","summary":"","tags":["Uncertainty Quantification","Anomaly Detection","Active Learning","Learning to Reject"],"title":"How to Allocate your Label Budget? Choosing between Active Learning and Learning to Reject in Anomaly Detection","type":"talk"},{"authors":["Vincent Vercruyssen","Lorenzo Perini","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1663113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663113600,"objectID":"8658861ec12c35323e6629b4d1954b9c","permalink":"https://lorenzoperini.github.io/publication/ecml2022/","publishdate":"2022-08-21T12:25:53+02:00","relpermalink":"/publication/ecml2022/","section":"publication","summary":" Active learning aims to ease the burden of collecting large amounts of annotated data by intelligently acquiring labels during the learning process that will be most helpful to learner. Current active learning approaches focus on learning from a single dataset. However, a common setting in practice requires simultaneously learning models from multiple datasets, where each dataset requires a separate learned model. This paper tackles the less-explored multi-domain active learning setting. We approach this from the perspective of multi-armed bandits and propose the active learning bandits (Alba) method, which uses bandit methods to both explore and exploit the usefulness of querying a label from different datasets in subsequent query rounds. We evaluate our approach on a benchmark of 7 datasets collected from a retail environment, in the context of a real-world use case of detecting anomalous resource usage. Alba outperforms existing active learning strategies, providing evidence that the standard active learning approaches are less suitable for the multi-domain setting.","tags":["Anomaly detection","Active Learning","Semi-Supervised Learning","Multi-Armed Bandit"],"title":"Multi-domain Active Learning for Semi-supervised Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"b117a9af064017a90f34de16d6ab26cb","permalink":"https://lorenzoperini.github.io/publication/aaai22/","publishdate":"2022-02-15T11:53:37+02:00","relpermalink":"/publication/aaai22/","section":"publication","summary":"Anomaly detection attempts to find examples in a dataset that do not conform to the expected behavior. Algorithms for this task assign an anomaly score to each example representing its degree of anomalousness. Setting a threshold on the anomaly scores enables converting these scores into a discrete prediction for each example. Setting an appropriate threshold is challenging in practice since anomaly detection is often treated as an unsupervised problem. A common approach is to set the threshold based on the dataset’s contamination factor, i.e., the proportion of anomalous examples in the data. While the contamination factor may be known based on domain knowledge, it is often necessary to estimate it by labeling data. However, many anomaly detection problems involve monitoring multiple related, yet slightly different entities (e.g., a fleet of machines). Then, estimating the contamination factor for each dataset separately by labeling data would be extremely time-consuming. Therefore, this paper introduces a method for transferring the known contamination factor from one dataset (the source domain) to a related dataset where it is unknown (the target domain). Our approach does not require labeled target data and is based on modeling the shape of the distribution of the anomaly scores in both domains. We theoretically analyze how our method behaves when the (biased) target domain anomaly score distribution converges to its true one. Empirically, our method outperforms several baselines on real-world datasets.","tags":["Transfer Learning","Unsupervised Learning","Anomaly Detection"],"title":"Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity","type":"publication"},{"authors":["Jonas Soenen","Elia Van Wolputte","Lorenzo Perini","Vincent Vercruyssen","Wannes Meert","Jesse Davis","Hendrik Blockeel"],"categories":[],"content":"","date":1625270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625270400,"objectID":"6564dc4c112d164dc939217606831b95","permalink":"https://lorenzoperini.github.io/publication/odd21/","publishdate":"2021-07-03T00:00:00Z","relpermalink":"/publication/odd21/","section":"publication","summary":"Anomaly detection aims at finding observations in a dataset that do not conform to expected behavior. Researchers have proposed a large variety of anomaly detection algorithms and their performance is greatly affected by how a user sets each algorithm’s hyperparameters. However, the anomaly detection literature does not agree on how to set these hyperparameters when experimentally comparing different algorithms. Most papers compare either performance using “default” settings, or maximal performance under optimal settings. In this paper, we argue that both strategies fail to capture what practitioners are actually interested in: how well does the algorithm perform in practice? They are either too pessimistic, assuming no tuning, or unrealistically optimistic, assuming optimal tuning; and they often result in methodologically unsound and irreproducible comparisons between algorithms. We therefore propose to use a small validation set to tune an anomaly detector’s hyperparameters on a per dataset basis. We argue this is realistic, striking the balance between keeping the cost of acquiring labeled data low and selecting the hyperparameters in a fair, sound, and reproducible manner. We provide a theoretical lower bound on the validation set size based on probability of an anomaly detector achieving a higher area under the ROC curve than a random detector. Using a benchmark of 16 datasets, we experimentally show that different hyperparameter selection strategies lead to different conclusions about which algorithms perform better than others, and that using a small validation set is a practically feasible and principled way of tuning the hyperparameters for a given dataset.","tags":["Data Mining","Anomaly Detection","Outlier Detection"],"title":"The Effect of Hyperparameter Tuning on the Comparative Evaluation of Unsupervised Anomaly Detection Methods","type":"publication"},{"authors":[],"categories":null,"content":"","date":1614616200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614616200,"objectID":"0f813efe7e3199da42662f1905bb4034","permalink":"https://lorenzoperini.github.io/talk/polito/","publishdate":"2021-03-01T17:30:00+01:00","relpermalink":"/talk/polito/","section":"talk","summary":"","tags":["Uncertainty Quantification","Anomaly Detection"],"title":"Quantifying the Confidence of Anomaly Detectors in Their Example-Wise Predictions","type":"talk"},{"authors":[],"categories":[],"content":"","date":1612998000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612998000,"objectID":"373ec5f4c4db1b82fe3f13e4d22704d2","permalink":"https://lorenzoperini.github.io/award/fwo_fellowship/","publishdate":"2021-02-11T00:00:00+01:00","relpermalink":"/award/fwo_fellowship/","section":"award","summary":"PhD grant for the research project ''Measuring and Exploiting the Uncertainty in Anomaly Detection''.","tags":[],"title":"PhD Fellowship fundamental research (FWO)","type":"award"},{"authors":[],"categories":[],"content":"","date":1612998000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612998000,"objectID":"f451c30dca3854993457fbb7574ace33","permalink":"https://lorenzoperini.github.io/award/sb_fellowship/","publishdate":"2021-02-11T00:00:00+01:00","relpermalink":"/award/sb_fellowship/","section":"award","summary":"F.R.S.-FNRS \u0026 FWO grant for talented PhD students for a long research stay abroad.","tags":[],"title":"Scientific prize Gustave Boël-Sofina Fellowship","type":"award"},{"authors":["Lorenzo Perini","Connor Galvin","Vincent Vercruyssen"],"categories":[],"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"a719ab9dd8683d5b8550a2853736de62","permalink":"https://lorenzoperini.github.io/publication/edml20/","publishdate":"2020-08-21T12:35:55+02:00","relpermalink":"/publication/edml20/","section":"publication","summary":"Anomaly detection attempts to learn models from data that can detect anomalous examples in the data. However, naturally occurring variations in the data impact the model that is learned and thus which examples it will predict to be anomalies. Ideally, an anomaly detection method should be robust to such small changes in the data. Hence, this paper introduces a ranking stability measure that quantifies the robustness of any anomaly detector's predictions by looking at how consistently it ranks examples in terms of their anomalousness. Our experiments investigate the performance of this stability measure under different data perturbation schemes. In addition, they show how the stability measure can complement traditional anomaly detection performance measures, such as area under the ROC curve or average precision, to quantify the behaviour of different anomaly detection methods.","tags":["Ranking Stability","Anomaly Detection","Classifier Trust"],"title":"A Ranking Stability Measure for Quantifying the Robustness of Anomaly Detection Methods","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"2ce36d5587162cbb092eaf84c4633b39","permalink":"https://lorenzoperini.github.io/publication/ecml20/","publishdate":"2020-08-21T12:25:53+02:00","relpermalink":"/publication/ecml20/","section":"publication","summary":"Anomaly detection focuses on identifying examples in the data that somehow deviate from what is expected or typical. Algorithms for this task usually assign a score to each example that represents how anomalous the example is. Then, a threshold on the scores turns them into concrete predictions. However, each algorithm uses a different approach to assign the scores, which makes them difficult to interpret and can quickly erode a user's trust in the predictions. This paper introduces an approach for assessing the reliability of any anomaly detector's example-wise predictions. To do so, we propose a Bayesian approach for converting anomaly scores to probability estimates. This enables the anomaly detector to assign a confidence score to each prediction which captures its uncertainty in that prediction. We theoretically analyze the convergence behaviour of our confidence estimate. Empirically, we demonstrate the effectiveness of the framework in quantifying a detector's confidence in its predictions on a large benchmark of datasets.","tags":["Anomaly Detection","Interpretability","Confidence Scores"],"title":"Quantifying the Confidence of Anomaly Detectors in Their Example-Wise Predictions","type":"publication"},{"authors":[],"categories":[],"content":"","date":1598256914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256914,"objectID":"d5ef38474751715cc3dea6d152772ce0","permalink":"https://lorenzoperini.github.io/course/pulearning/","publishdate":"2020-08-24T10:15:14+02:00","relpermalink":"/course/pulearning/","section":"course","summary":"2020/2021, 2021/2022","tags":[],"title":"Capita Selecta Computer Science: PU Learning","type":"course"},{"authors":[],"categories":[],"content":"","date":1598256914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256914,"objectID":"bdba931ff2a5f4a110fe4bbb0d893e9f","permalink":"https://lorenzoperini.github.io/course/datamining/","publishdate":"2020-08-24T10:15:14+02:00","relpermalink":"/course/datamining/","section":"course","summary":"2019/2020, 2020/2021, 2022/2023","tags":[],"title":"Data Mining","type":"course"},{"authors":[],"categories":[],"content":"","date":1598256676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256676,"objectID":"498d13916311804fdf6ebcf150c6f020","permalink":"https://lorenzoperini.github.io/award/ecmlpkdd20/","publishdate":"2020-08-24T10:11:16+02:00","relpermalink":"/award/ecmlpkdd20/","section":"award","summary":"Multiple nominations by the conference session chairs as particularly engaging speaker.","tags":[],"title":"Overall ECML-PKDD Engagement Award 2020","type":"award"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"300e9157a6182156b89161d233b14e6b","permalink":"https://lorenzoperini.github.io/publication/ijcai20/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/publication/ijcai20/","section":"publication","summary":"Estimating the proportion of positive examples (i.e., the class prior) from positive and unlabeled (PU) data is an important task that facilitates learning a classifier from such data. In this paper, we explore how to tackle this problem when the observed labels were acquired via active learning. This introduces the challenge that the observed labels were not selected completely at random, which is the primary assumption underpinning existing approaches to estimating the class prior from PU data. We analyze this new setting and design an algorithm that is able to estimate the class prior for a given active learning strategy. Empirically, we show that our approach accurately recovers the true class prior on a benchmark of anomaly detection datasets and that it does so more accurately than existing methods.","tags":["PU Learning","Active Learning","Anomaly Detection"],"title":"Class Prior Estimation in Active Positive and Unlabeled Learning","type":"publication"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d32de50d40e4fd5995af286531e188cb","permalink":"https://lorenzoperini.github.io/thesis/1year/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/1year/","section":"thesis","summary":"","tags":[],"title":"[2019/2020] Designing a Stability Metric for Assessing the Robustness of Anomaly Rankings","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e654b6c3183702c343954e9df7cce8b7","permalink":"https://lorenzoperini.github.io/thesis/2year/first/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/2year/first/","section":"thesis","summary":"","tags":[],"title":"[2020/2021] Do you know the answer? Taking into account the user uncertainty in active learning","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa67618809b1f7e5f33abdbafc04fb70","permalink":"https://lorenzoperini.github.io/thesis/2year/second/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/2year/second/","section":"thesis","summary":"","tags":[],"title":"[2020/2021] Reliability measure in the Active Learning querying phase","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f36bbe2cec6cb0d8ea86bbc69b349c2a","permalink":"https://lorenzoperini.github.io/thesis/3year/second/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/3year/second/","section":"thesis","summary":"","tags":[],"title":"[2021/2022] Adaptive semi-supervised anomaly detection with any unsupervised prior","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0b09fd5d1350d3aa76076bcd1d473560","permalink":"https://lorenzoperini.github.io/thesis/3year/third/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/3year/third/","section":"thesis","summary":"","tags":[],"title":"[2021/2022] Practice makes perfect? Detecting anomalies by learning from imperfect user’s feedback","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c365661862aebbc874ee0ade697c817","permalink":"https://lorenzoperini.github.io/thesis/3year/first/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/3year/first/","section":"thesis","summary":"","tags":[],"title":"[2021/2022] To ask or to abstain, what is the best strategy? Finding the best trade-off between Active Learning and Learning to Reject","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"51d909a1826efd5c38b31410becffbc0","permalink":"https://lorenzoperini.github.io/thesis/4year/second/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/4year/second/","section":"thesis","summary":"","tags":[],"title":"[2022/2023] It is likely not to be so likely! Semi-supervised calibration of anomaly scores","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4dadbcfa2174f748c8799cd9ac2edc51","permalink":"https://lorenzoperini.github.io/thesis/4year/first/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/4year/first/","section":"thesis","summary":"","tags":[],"title":"[2022/2023] This is critical and a lot is at stake. How can I trust the model? Quantifying the model uncertainty in anomaly detection","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2149f0c88b157e0f8ec0707bfa3810de","permalink":"https://lorenzoperini.github.io/thesis/5year/second/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/5year/second/","section":"thesis","summary":"","tags":[],"title":"[2023/2024] Better be cautious when asking hard questions! Developing an active learning strategy for semi-supervised models with rejection.","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d1524173b6de84da026d42a261c152b","permalink":"https://lorenzoperini.github.io/thesis/5year/third/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/5year/third/","section":"thesis","summary":"","tags":[],"title":"[2023/2024] It may not be wrong but is definitely more anomalous than the others! Anomaly Detection with User Feedback Ranking","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"46fedc928ae6b2e933e22c3101b9b608","permalink":"https://lorenzoperini.github.io/thesis/5year/first/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/5year/first/","section":"thesis","summary":"","tags":[],"title":"[2023/2024] What do slot machines have in common with Active Learning? Finding the high-reward instances in Multi-Instance Learning.","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"256f085db9b42d9be9674b4669f81f40","permalink":"https://lorenzoperini.github.io/reviewer/aaai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/aaai/","section":"reviewer","summary":"Conference","tags":[],"title":"AAAI Conference on Artificial Intelligence (AAAI-21, AAAI-22)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbba21c7d05c83ac5e4303514dc65875","permalink":"https://lorenzoperini.github.io/reviewer/kdd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/kdd/","section":"reviewer","summary":"Conference","tags":[],"title":"ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-21, KDD-23)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6bad1e87e8fced13250c39179ae29bc2","permalink":"https://lorenzoperini.github.io/reviewer/mlj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/mlj/","section":"reviewer","summary":"Journal","tags":[],"title":"Editorial Board Member of Machine Learning Journal (MLJ)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5cb30cd5335bb5d05f5ef321ea7b0308","permalink":"https://lorenzoperini.github.io/reviewer/ecml/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/ecml/","section":"reviewer","summary":"Conference","tags":[],"title":"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-20, ECML-22)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8b392075b975480f7a4cc2257f031f7e","permalink":"https://lorenzoperini.github.io/reviewer/aistats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/aistats/","section":"reviewer","summary":"Conference","tags":[],"title":"International Conference on Artificial Intelligence and Statistics (AISTATS-23)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af4491e277020e9d3b1708989d41e20d","permalink":"https://lorenzoperini.github.io/reviewer/jair/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/jair/","section":"reviewer","summary":"","tags":[],"title":"Journal of Artificial Intelligence Research (JAIR)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e73f7dce02f6a8a47993dd53efcde13","permalink":"https://lorenzoperini.github.io/reviewer/neurips/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/neurips/","section":"reviewer","summary":"Conference","tags":[],"title":"Neural Information Processing Systems (NeurIPS-24)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"141c50570e8a05106b89515f08c1c5b6","permalink":"https://lorenzoperini.github.io/reviewer/sdm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/sdm/","section":"reviewer","summary":"Conference","tags":[],"title":"SIAM International Conference on Data Mining (SDM-22)","type":"reviewer"}]